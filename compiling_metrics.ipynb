{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf44f9e5-a505-4562-b8e0-e337dd4cb8f1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/figenio/Projetos/data_science/experimentacao/graph_experimenting/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset, features, ClassLabel, load_from_disk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import researchpy as rp\n",
    "from itertools import chain\n",
    "\n",
    "import networkx as nx\n",
    "import community as community_louvain\n",
    "from networkx.algorithms.community import k_clique_communities\n",
    "from networkx.algorithms import bipartite\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot\n",
    "from networkx.drawing.nx_pydot import graphviz_layout\n",
    "\n",
    "from llm_mri import LLM_MRI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8125f08-83c8-4725-a308-376999be7607",
   "metadata": {},
   "source": [
    "# Classe de métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f8a42e-e83f-4636-9ba2-3a158e45cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM_metrics:\n",
    "\n",
    "    def __init__(self, Graph, label, model_name, map_dimensions, total_num_of_layers):\n",
    "        self.Graph = Graph\n",
    "        self.label = label\n",
    "        self.model_name = model_name\n",
    "        self.map_dimensions = map_dimensions\n",
    "        self.layers = total_num_of_layers\n",
    "\n",
    "        \"\"\"\n",
    "        Rotulando cada nó com a camada que ele pertence, isso é feito\n",
    "        buscando o primeiro número do seu nome\n",
    "        \"\"\"\n",
    "        for n in self.Graph.nodes:\n",
    "            self.Graph.nodes[n]['layer'] = int(n.split('_')[0])\n",
    "        \n",
    "        self.projection_even, self.projection_odd = self.project_graph()\n",
    "\n",
    "    def project_graph(self):\n",
    "        nodes_even_layers = set()\n",
    "        nodes_odd_layers = set()\n",
    "        \"\"\"\n",
    "        Para fazer a projeção, é necessário que os nós tenham a label\n",
    "        de qual camada pertencem.\n",
    "        \"\"\"\n",
    "        for layer in range(self.layers + 1):\n",
    "            if layer % 2 == 0:\n",
    "                nodes_even_layers = nodes_even_layers.union({ n for n, d in self.Graph.nodes(data=True) if d['layer'] == layer})\n",
    "            else:\n",
    "                nodes_odd_layers = nodes_odd_layers.union({ n for n, d in self.Graph.nodes(data=True) if d['layer'] == layer})\n",
    "\n",
    "        return bipartite.collaboration_weighted_projected_graph(self.Graph, nodes_even_layers), bipartite.collaboration_weighted_projected_graph(self.Graph, nodes_odd_layers)\n",
    "\n",
    "    def get_degree_by_layer(self):\n",
    "        camadas = []\n",
    "        for x in range(self.layers + 1):\n",
    "            camadas.append(str(x))\n",
    "        df_layers = pd.DataFrame(columns=['layer', 'mean', 'var'])\n",
    "\n",
    "        for i in camadas:\n",
    "            df_layers = pd.concat([pd.DataFrame([[\n",
    "                i,\n",
    "                pd.Series([v for k, v in dict(nx.degree(self.Graph)).items() if k.split(\"_\")[0] == i]).mean(),\n",
    "                pd.Series([v for k, v in dict(nx.degree(self.Graph)).items() if k.split(\"_\")[0] == i]).var(),\n",
    "            ]], columns=df_layers.columns), df_layers], ignore_index=True)\n",
    "        \n",
    "        return df_layers.reindex(index=df_layers.index[::-1])\n",
    "\n",
    "    def get_graph_center_of_mass(self):\n",
    "        camadas = []\n",
    "        for x in range(self.layers + 1):\n",
    "            camadas.append(str(x))\n",
    "        \n",
    "        center_of_mass = 0\n",
    "\n",
    "        for i in camadas:\n",
    "            center_of_mass += ((pd.Series([k for k, v in dict(self.Graph.nodes()).items() if k.split(\"_\")[0] == i]).count()) * (int(i) - (self.layers / 2)))\n",
    "        \n",
    "        return center_of_mass / len(list(self.Graph.nodes()))\n",
    "\n",
    "    def get_graph_center_of_strength(self):\n",
    "        camadas = []\n",
    "        for x in range(self.layers + 1):\n",
    "            camadas.append(str(x))\n",
    "        \n",
    "        center_of_strength = 0\n",
    "        # sum_of_var = 0\n",
    "        array_of_strenght = []\n",
    "        sum_of_weights = 0\n",
    "\n",
    "        for i in camadas:\n",
    "            center_of_strength += ((pd.Series([v for k, v in dict(self.Graph.degree(weight='weight')).items() if k.split(\"_\")[0] == i]).std()) * (int(i) - (self.layers / 2)))\n",
    "            # array_of_strenght.append(pd.Series([v for k, v in dict(self.Graph.degree(weight='weight')).items() if k.split(\"_\")[0] == i]).std())\n",
    "            # sum_of_var += abs(((pd.Series([v for k, v in dict(self.Graph.degree(weight='weight')).items() if k.split(\"_\")[0] == i]).var()) * (int(i) - (self.layers / 2))))\n",
    "            # sum_of_weights += pd.Series([v for k, v in dict(self.Graph.degree(weight='weight')).items() if k.split(\"_\")[0] == i]).sum()\n",
    "        \n",
    "        return center_of_strength\n",
    "        # return center_of_strength / sum_of_weights\n",
    "        # return center_of_strength / sum_of_var\n",
    "        # return pd.Series(array_of_strenght).mean()\n",
    "\n",
    "    def get_graph(self):\n",
    "        return self.Graph\n",
    "    \n",
    "    def get_basic_metrics(self):\n",
    "        return {\n",
    "            \"mean_degree\": pd.Series([v for k, v in dict(nx.degree(self.Graph)).items()]).mean(),\n",
    "            \"var_degree\": pd.Series([v for k, v in dict(nx.degree(self.Graph)).items()]).var(),\n",
    "            \"skew_degree\": pd.Series([v for k, v in dict(nx.degree(self.Graph)).items()]).skew(),\n",
    "            \"kurt_degree\": pd.Series([v for k, v in dict(nx.degree(self.Graph)).items()]).kurt(),\n",
    "            \"mean_strength\": pd.Series([v for k, v in dict(self.Graph.degree(weight='weight')).items()]).mean(),\n",
    "            \"var_strength\": pd.Series([v for k, v in dict(self.Graph.degree(weight='weight')).items()]).var(),\n",
    "            \"skew_strength\": pd.Series([v for k, v in dict(self.Graph.degree(weight='weight')).items()]).skew(),\n",
    "            \"kurt_strength\": pd.Series([v for k, v in dict(self.Graph.degree(weight='weight')).items()]).kurt(),\n",
    "            # \"average_node_connectivity\": nx.average_node_connectivity(self.Graph),\n",
    "            \"assortativity\": nx.degree_assortativity_coefficient(self.Graph, weight='weight'),\n",
    "            \"density\": nx.density(self.Graph),\n",
    "            \"center_of_mass\": self.get_graph_center_of_mass(),\n",
    "            \"center_of_strength\": self.get_graph_center_of_strength(),\n",
    "            \"model_name\": self.model_name,\n",
    "            \"map_dimensions\": self.map_dimensions,\n",
    "            \"label\": self.label\n",
    "        }\n",
    "\n",
    "    def get_projection_metrics_even(self):\n",
    "        return {\n",
    "            \"mean_degree\": pd.Series([v for k, v in dict(nx.degree(self.projection_even)).items()]).mean(),\n",
    "            \"var_degree\": pd.Series([v for k, v in dict(nx.degree(self.projection_even)).items()]).var(),\n",
    "            \"mean_strength\": pd.Series([v for k, v in dict(self.projection_even.degree(weight='weight')).items()]).mean(),\n",
    "            \"var_strength\": pd.Series([v for k, v in dict(self.projection_even.degree(weight='weight')).items()]).var(),\n",
    "            \"average_clustering\": nx.average_clustering(self.projection_even, weight=\"weight\"),\n",
    "            # \"average_node_connectivity\": nx.average_node_connectivity(self.projection_even),\n",
    "            \"assortativity\": nx.degree_assortativity_coefficient(self.projection_even, weight='weight'),\n",
    "            \"density\": nx.density(self.projection_even),\n",
    "            \"average_shortest_path\": nx.average_shortest_path_length(self.projection_even, weight=\"weight\") if nx.is_connected(self.projection_even) else float('NaN'),\n",
    "            \"model_name\": self.model_name,\n",
    "            \"map_dimensions\": self.map_dimensions,\n",
    "            \"label\": self.label,\n",
    "            \"side\": \"even\"\n",
    "        }\n",
    "\n",
    "    def get_projection_metrics_odd(self):\n",
    "        return {\n",
    "            \"mean_degree\": pd.Series([v for k, v in dict(nx.degree(self.projection_odd)).items()]).mean(),\n",
    "            \"var_degree\": pd.Series([v for k, v in dict(nx.degree(self.projection_odd)).items()]).var(),\n",
    "            \"mean_strength\": pd.Series([v for k, v in dict(self.projection_odd.degree(weight='weight')).items()]).mean(),\n",
    "            \"var_strength\": pd.Series([v for k, v in dict(self.projection_odd.degree(weight='weight')).items()]).var(),\n",
    "            \"average_clustering\": nx.average_clustering(self.projection_odd, weight=\"weight\"),\n",
    "            # \"average_node_connectivity\": nx.average_node_connectivity(self.projection_odd),\n",
    "            \"assortativity\": nx.degree_assortativity_coefficient(self.projection_odd, weight=\"weight\"),\n",
    "            \"density\": nx.density(self.projection_odd),\n",
    "            \"average_shortest_path\": nx.average_shortest_path_length(self.projection_odd, weight=\"weight\") if nx.is_connected(self.projection_odd) else float('NaN'),\n",
    "            \"model_name\": self.model_name,\n",
    "            \"map_dimensions\": self.map_dimensions,\n",
    "            \"label\": self.label,\n",
    "            \"side\": \"odd\"\n",
    "        }\n",
    "\n",
    "    def get_basic_metrics_list_of_names(self):\n",
    "        return [\n",
    "            'mean_degree',\n",
    "            'var_degree',\n",
    "            'skew_degree',\n",
    "            'kurt_degree',\n",
    "            \"mean_strength\",\n",
    "            \"var_strength\",\n",
    "            \"skew_strength\",\n",
    "            \"kurt_strength\",\n",
    "            'average_clustering',\n",
    "            # 'average_node_connectivity',\n",
    "            'assortativity',\n",
    "            'density',\n",
    "            'model_name',\n",
    "            'map_dimensions',\n",
    "            'label',\n",
    "            'side',\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3a17cb-2ead-4275-a579-c45390a9fa1d",
   "metadata": {},
   "source": [
    "# Analisando Ativações do Grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6e52239-4c94-48a4-a0e3-76d1c1117bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G_top = nx.read_gml('graphs/dpoc_bert-base-portuguese-cased_10_top.gml')\n",
    "# G_botton = nx.read_gml('graphs/dpoc_bert-base-portuguese-cased_10_botton.gml')\n",
    "# top_metrics = LLM_metrics(G_top, 'top', 'bert-base-portuguese-cased', 10, 12)\n",
    "# top_metrics.get_basic_metrics()\n",
    "\n",
    "# camadas = []\n",
    "# for x in range(12):\n",
    "#     camadas.append(str(x))\n",
    "# center_of_edge = 0\n",
    "# top_edges_by_layer = []\n",
    "# for i in camadas:\n",
    "#     center_of_edge += ((pd.Series([v['weight'] for k, v in dict(G_top.edges()).items() if (k[0].split(\"_\")[0] == str(int(i)+1) and  k[1].split(\"_\")[0] == str(int(i)+1)) or (k[1].split(\"_\")[0] == str(int(i)+1) and  k[0].split(\"_\")[0] == str(int(i)+1))]).sum()) * (int(i) - (12 / 2)))\n",
    "#     top_edges_by_layer.append(pd.Series([v['weight'] for k, v in dict(G_top.edges()).items() if (k[0].split(\"_\")[0] == i and  k[1].split(\"_\")[0] == str(int(i)+1)) or (k[1].split(\"_\")[0] == i and  k[0].split(\"_\")[0] == str(int(i)+1))]).sum())\n",
    "#     print(((pd.Series([v['weight'] for k, v in dict(G_top.edges()).items() if (k[0].split(\"_\")[0] == i and  k[1].split(\"_\")[0] == str(int(i)+1)) or (k[1].split(\"_\")[0] == i and  k[0].split(\"_\")[0] == str(int(i)+1))]).sum())))\n",
    "# print(\"centro\", center_of_edge)\n",
    "# center_of_edge = 0\n",
    "# top_edges_by_layer = []\n",
    "# for i in camadas:\n",
    "#     center_of_edge += ((pd.Series([v['weight'] for k, v in dict(G_botton.edges()).items() if (k[0].split(\"_\")[0] == str(int(i)+1) and  k[1].split(\"_\")[0] == str(int(i)+1)) or (k[1].split(\"_\")[0] == str(int(i)+1) and  k[0].split(\"_\")[0] == str(int(i)+1))]).sum()) * (int(i) - (12 / 2)))\n",
    "#     top_edges_by_layer.append(pd.Series([v['weight'] for k, v in dict(G_botton.edges()).items() if (k[0].split(\"_\")[0] == i and  k[1].split(\"_\")[0] == str(int(i)+1)) or (k[1].split(\"_\")[0] == i and  k[0].split(\"_\")[0] == str(int(i)+1))]).sum())\n",
    "#     print(((pd.Series([v['weight'] for k, v in dict(G_botton.edges()).items() if (k[0].split(\"_\")[0] == i and  k[1].split(\"_\")[0] == str(int(i)+1)) or (k[1].split(\"_\")[0] == i and  k[0].split(\"_\")[0] == str(int(i)+1))]).sum())))\n",
    "# print(\"centro\", center_of_edge)\n",
    "# center_of_edge/pd.Series([v['weight'] for k, v in dict(G_top.edges()).items()]).sum()\n",
    "# top_edges_by_layer\n",
    "# pd.Series([v['weight'] for k, v in dict(G_top.edges()).items()]).\n",
    "# plt.scatter(camadas,top_edges_by_layer)\n",
    "# import math \n",
    "# camadas = []\n",
    "# for x in range(12 + 1):\n",
    "#     camadas.append(str(x))\n",
    "# center_of_strenght = 0\n",
    "# sum_var = 0\n",
    "# array_of_strenght = []\n",
    "# for i in camadas:\n",
    "#     center_of_strenght += ((pd.Series([v for k, v in dict(G_top.degree(weight='weight')).items() if k.split(\"_\")[0] == i]).std()) * (int(i) - (12 / 2)))\n",
    "#     array_of_strenght.append(((pd.Series([v for k, v in dict(G_top.degree(weight='weight')).items() if k.split(\"_\")[0] == i]).std()) * (int(i) - (12 / 2))))\n",
    "#     sum_var += abs(((pd.Series([v for k, v in dict(G_top.degree(weight='weight')).items() if k.split(\"_\")[0] == i]).std()) * (int(i) - (12 / 2))))\n",
    "#     # print(center_of_strenght)\n",
    "#     # print('momento:', ((pd.Series([v for k, v in dict(G_top.degree(weight='weight')).items() if k.split(\"_\")[0] == i]).var())))\n",
    "# pd.Series(array_of_strenght).mean()\n",
    "# print(\"Centro of std:\", center_of_strenght)\n",
    "# print(\"Centro por soma de vairância:\", center_of_strenght / sum_var)\n",
    "# print(\"Centro por soma total:\", center_of_strenght/pd.Series([v for k, v in dict(G_top.degree(weight='weight')).items() if k.split(\"_\")[0] == i]).sum())\n",
    "# print(\"Centro por soma média:\", center_of_strenght/pd.Series([v for k, v in dict(G_top.degree(weight='weight')).items() if k.split(\"_\")[0] == i]).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41e52001-5881-4431-9acd-fa47956f4f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics: iam  -  neuralmind/bert-base-portuguese-cased  -  5  -  12\n",
      "Getting metrics: iam  -  neuralmind/bert-base-portuguese-cased  -  10  -  12\n",
      "Getting metrics: iam  -  neuralmind/bert-base-portuguese-cased  -  25  -  12\n",
      "Getting metrics: iam  -  pucpr/biobertpt-all  -  5  -  12\n",
      "Getting metrics: iam  -  pucpr/biobertpt-all  -  10  -  12\n",
      "Getting metrics: iam  -  pucpr/biobertpt-all  -  25  -  12\n",
      "Getting metrics: iam  -  google-bert/bert-base-multilingual-cased  -  5  -  12\n",
      "Getting metrics: iam  -  google-bert/bert-base-multilingual-cased  -  10  -  12\n",
      "Getting metrics: iam  -  google-bert/bert-base-multilingual-cased  -  25  -  12\n",
      "Getting metrics: iam  -  neuralmind/bert-large-portuguese-cased  -  5  -  24\n",
      "Getting metrics: iam  -  neuralmind/bert-large-portuguese-cased  -  10  -  24\n",
      "Getting metrics: iam  -  neuralmind/bert-large-portuguese-cased  -  25  -  24\n",
      "Getting metrics: iam  -  pierreguillou/gpt2-small-portuguese  -  5  -  12\n",
      "Getting metrics: iam  -  pierreguillou/gpt2-small-portuguese  -  10  -  12\n",
      "Getting metrics: iam  -  pierreguillou/gpt2-small-portuguese  -  25  -  12\n",
      "Getting metrics: iam  -  pucpr/gpt2-bio-pt  -  5  -  12\n",
      "Getting metrics: iam  -  pucpr/gpt2-bio-pt  -  10  -  12\n",
      "Getting metrics: iam  -  pucpr/gpt2-bio-pt  -  25  -  12\n",
      "Getting metrics: dpoc  -  neuralmind/bert-base-portuguese-cased  -  5  -  12\n",
      "Getting metrics: dpoc  -  neuralmind/bert-base-portuguese-cased  -  10  -  12\n",
      "Getting metrics: dpoc  -  neuralmind/bert-base-portuguese-cased  -  25  -  12\n",
      "Getting metrics: dpoc  -  pucpr/biobertpt-all  -  5  -  12\n",
      "Getting metrics: dpoc  -  pucpr/biobertpt-all  -  10  -  12\n",
      "Getting metrics: dpoc  -  pucpr/biobertpt-all  -  25  -  12\n",
      "Getting metrics: dpoc  -  google-bert/bert-base-multilingual-cased  -  5  -  12\n",
      "Getting metrics: dpoc  -  google-bert/bert-base-multilingual-cased  -  10  -  12\n",
      "Getting metrics: dpoc  -  google-bert/bert-base-multilingual-cased  -  25  -  12\n",
      "Getting metrics: dpoc  -  neuralmind/bert-large-portuguese-cased  -  5  -  24\n",
      "Getting metrics: dpoc  -  neuralmind/bert-large-portuguese-cased  -  10  -  24\n",
      "Getting metrics: dpoc  -  neuralmind/bert-large-portuguese-cased  -  25  -  24\n",
      "Getting metrics: dpoc  -  pierreguillou/gpt2-small-portuguese  -  5  -  12\n",
      "Getting metrics: dpoc  -  pierreguillou/gpt2-small-portuguese  -  10  -  12\n",
      "Getting metrics: dpoc  -  pierreguillou/gpt2-small-portuguese  -  25  -  12\n",
      "Getting metrics: dpoc  -  pucpr/gpt2-bio-pt  -  5  -  12\n",
      "Getting metrics: dpoc  -  pucpr/gpt2-bio-pt  -  10  -  12\n",
      "Getting metrics: dpoc  -  pucpr/gpt2-bio-pt  -  25  -  12\n"
     ]
    }
   ],
   "source": [
    "# exp = \"iam\"\n",
    "# exp = \"dpoc\"\n",
    "experiments = [\n",
    "    \"iam\",\n",
    "    \"dpoc\"\n",
    "]\n",
    "\n",
    "lang = \"pt\"\n",
    "# # lang = \"en\"\n",
    "\n",
    "map_dimensions = [\n",
    "    5,\n",
    "    10,\n",
    "    25\n",
    "]\n",
    "\n",
    "models = { \n",
    "    # \"nlpie/tiny-clinicalbert\": 4,          \n",
    "    # \"distilbert-base-uncased\": 6,\n",
    "    # \"google-bert/bert-base-uncased\": 12,\n",
    "    # \"emilyalsentzer/Bio_ClinicalBERT\": 12,\n",
    "    # \"google-bert/bert-large-uncased\": 24,\n",
    "\n",
    "    \"neuralmind/bert-base-portuguese-cased\": 12, # 110M\n",
    "    \"pucpr/biobertpt-all\": 12, # 110M\n",
    "    \"google-bert/bert-base-multilingual-cased\": 12, # 110M\n",
    "    \"neuralmind/bert-large-portuguese-cased\": 24, # 330M\n",
    "    \n",
    "    # \"openai-community/gpt2\": 12,\n",
    "    # \"openai-community/gpt2-large\": 36\n",
    "    # \"FacebookAI/xlm-roberta-large\": 24,\n",
    "    # \"facebook/xlm-roberta-xl\": 36,          # 3.48B\n",
    "\n",
    "    \"pierreguillou/gpt2-small-portuguese\": 12, # 1.5B\n",
    "    \"pucpr/gpt2-bio-pt\": 12, # 1.5B\n",
    "}\n",
    "\n",
    "\n",
    "# number_of_layers = 1 + list(models.items())[position][1]\n",
    "\n",
    "# df_basic = pd.DataFrame(columns = ['mean_degree','var_degree','average_node_connectivity','assortativity','density','center_of_mass','model_name','map_dimensions','label'])\n",
    "# df_projection = pd.DataFrame(columns = ['mean_degree','var_degree','average_clustering','average_node_connectivity','assortativity','density','model_name','map_dimensions','label','side'])\n",
    "# df_basic = pd.DataFrame()\n",
    "# df_projection = pd.DataFrame()\n",
    "\n",
    "# df_basic = pd.read_csv('data/comparison_basic_metrics' + lang + exp + '.csv')\n",
    "# df_projection = pd.read_csv('data/comparison_projection_metrics' + lang + exp + '.csv')\n",
    "\n",
    "for exp in experiments:\n",
    "    df_basic = pd.DataFrame()\n",
    "    df_projection = pd.DataFrame()\n",
    "    \n",
    "    for model_name, number_of_layers in models.items():\n",
    "        for dimension in map_dimensions:\n",
    "            print(\"Getting metrics:\", exp, \" - \", model_name, \" - \", dimension, \" - \", number_of_layers)\n",
    "            \n",
    "            G_top = nx.read_gml('graphs/' + exp + '_' + model_name.split('/')[1] + '_' + str(dimension) + '_top.gml')\n",
    "            G_botton = nx.read_gml('graphs/' + exp + '_' + model_name.split('/')[1] + '_' + str(dimension) + '_botton.gml')\n",
    "            G_composed = nx.read_gml('graphs/' + exp + '_' + model_name.split('/')[1] + '_' + str(dimension) + '_composed.gml')\n",
    "            \n",
    "            top_metrics = LLM_metrics(G_top, 'top', model_name.split('/')[-1], dimension, number_of_layers)\n",
    "            botton_metrics = LLM_metrics(G_botton, 'botton',model_name.split('/')[-1], dimension, number_of_layers)\n",
    "    \n",
    "            df_basic = pd.concat([pd.DataFrame.from_dict([\n",
    "                top_metrics.get_basic_metrics(),\n",
    "                botton_metrics.get_basic_metrics()\n",
    "            ]), df_basic], ignore_index=True)\n",
    "            \n",
    "            df_projection = pd.concat([pd.DataFrame.from_dict([\n",
    "                top_metrics.get_projection_metrics_even(),\n",
    "                top_metrics.get_projection_metrics_odd(),\n",
    "                botton_metrics.get_projection_metrics_even(),\n",
    "                botton_metrics.get_projection_metrics_odd()\n",
    "            ]), df_projection], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "df_basic.to_csv('data/comparison_basic_metrics_pt_' + exp + '.csv', index=False)\n",
    "df_projection.to_csv('data/comparison_projection_metrics_pt_' + exp + '.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
