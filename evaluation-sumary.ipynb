{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4390223-e903-4f7a-b63d-91f051d4524c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/figenio/Projetos/data_science/experimentacao/graph_experimenting/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset, features, ClassLabel, load_from_disk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import researchpy as rp\n",
    "from itertools import chain\n",
    "\n",
    "import networkx as nx\n",
    "import community as community_louvain\n",
    "from networkx.algorithms.community import k_clique_communities\n",
    "from networkx.algorithms import bipartite\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot\n",
    "from networkx.drawing.nx_pydot import graphviz_layout\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from stargazer.stargazer import Stargazer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from llm_mri import LLM_MRI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4ef9c0-e8d0-48cd-a1c0-bedc77d99d63",
   "metadata": {},
   "source": [
    "# Carregando dados e configurando análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ac7b4a9-2019-4bb3-82c5-793d0f83a864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top quantil: 6.0\n",
      "Botton quantil: 4.0\n",
      "All columns ['annotation id', 'original', 'auto', 'organization level', 'global score', 'label']\n",
      "top: 97\n",
      "botton: 81\n",
      "TOTAL: 447\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acute myocardial infarction presents an epide...</td>\n",
       "      <td>top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acute myocardial infarction (AMI) is a transmu...</td>\n",
       "      <td>top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ANGINOSE EQUIVALENTS: VOMITING, SWEATING, ATY...</td>\n",
       "      <td>mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMI is a disease that affects the heart muscle...</td>\n",
       "      <td>mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acute myocardial infarction is one of the main...</td>\n",
       "      <td>mid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0   Acute myocardial infarction presents an epide...   top\n",
       "1  Acute myocardial infarction (AMI) is a transmu...   top\n",
       "2   ANGINOSE EQUIVALENTS: VOMITING, SWEATING, ATY...   mid\n",
       "3  AMI is a disease that affects the heart muscle...   mid\n",
       "4  Acute myocardial infarction is one of the main...   mid"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def label_answer(row, score, botton_quantile, top_quantile):\n",
    "    if (row[score] < botton_quantile):\n",
    "        return 'botton'\n",
    "    elif (row[score] > top_quantile):\n",
    "        return 'top'\n",
    "    else:\n",
    "        return 'mid'\n",
    "        \n",
    "exp = \"_iam\"\n",
    "# exp = \"_dpoc\"\n",
    "lang = \"_pt\"\n",
    "# lang = \"_en\"\n",
    "if \"pt\" in lang:\n",
    "    text_column = 'original'\n",
    "else:\n",
    "    text_column = 'auto'\n",
    "df_traducao = pd.read_csv('data/experimento_consolidado' + exp + '.csv')\n",
    "\n",
    "score = 'global score'#organization_level,global_score\n",
    "quantile = .3\n",
    "\n",
    "print(\"Top quantil:\", df_traducao[score].quantile(1 - quantile))\n",
    "print(\"Botton quantil:\", df_traducao[score].quantile(quantile))\n",
    "\n",
    "#df_traducao[score].hist()\n",
    "\n",
    "df_traducao['label'] = df_traducao.apply(\n",
    "    label_answer,\n",
    "    axis=1,\n",
    "    score=score,\n",
    "    botton_quantile=df_traducao[score].quantile(quantile),\n",
    "    top_quantile=df_traducao[score].quantile(1 - quantile),\n",
    ")\n",
    "\n",
    "columns_to_remove = list(df_traducao.columns)\n",
    "print(\"All columns\", columns_to_remove)\n",
    "columns_to_remove.remove(text_column)\n",
    "columns_to_remove.remove('label')\n",
    "\n",
    "df_data = df_traducao.rename(columns={text_column: 'text'}).drop(columns=columns_to_remove).reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_data['label'].hist()\n",
    "# df_data = df_data.drop(df_data[df_data['label'] == \"mid\"].index).reset_index(drop=True)\n",
    "print(\"top:\", df_data['label'].loc[df_data['label'] == \"top\"].count())\n",
    "print(\"botton:\", df_data['label'].loc[df_data['label'] == \"botton\"].count())\n",
    "print(\"TOTAL:\", df_data['label'].count())\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9331377-d998-42d6-9f56-961c9ba0f8ed",
   "metadata": {},
   "source": [
    "# Passando os Dados para o Módulo\n",
    "\n",
    "- converte o Dataframe para Dataset;\n",
    "- carrega o modelo e os dados no módulo;\n",
    "- processa as ativações;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "521ae153-f1a3-4890-9ffa-e610d6422f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 447/447 [00:00<00:00, 20192.50 examples/s]\n",
      "Casting the dataset: 100%|█████████| 447/447 [00:00<00:00, 281214.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "subset = Dataset.from_pandas(df_data)\n",
    "subset.cleanup_cache_files()\n",
    "\n",
    "label_feature = subset.features['label']\n",
    "class_names  = subset.unique(\"label\")\n",
    "\n",
    "class_feature = features.ClassLabel(names=sorted(class_names))\n",
    "subset = subset.map(lambda str_value: {\"label\": class_feature.str2int(str_value)}, input_columns=\"label\")\n",
    "\n",
    "subset = subset.cast(features.Features({\n",
    "    \"label\": class_feature,\n",
    "    \"text\": subset.features[\"text\"]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecccba7-4971-4b95-a955-0182bf6e5204",
   "metadata": {},
   "source": [
    "# Definindo Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5563bc70-5621-475d-b344-116ee0c10f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping(array):\n",
    "    df_map = pd.DataFrame(array)\n",
    "    #df_model['norm_label']=(df_model['label']-df_model['label'].min())/(df_model['label'].max()-df_model['label'].min())\n",
    "    \n",
    "    cell_numeration = {}\n",
    "    \n",
    "    for idx, data in enumerate(llm_mri.reduced_dataset):\n",
    "        df_mid = data[['label','cell_label']].pivot_table(index=data.index, columns=\"cell_label\")\n",
    "        df_mid.columns = df_mid.columns.droplevel(0)\n",
    "    \n",
    "        cell_numeration[idx] = [df_map.columns.size, df_map.columns.size + df_mid.columns.size]\n",
    "        \n",
    "        df_map = pd.concat([\n",
    "            df_map,\n",
    "            df_mid\n",
    "        ], axis=1)\n",
    "    \n",
    "    # Filtra as duas classes de interesse\n",
    "    df_map = df_map[(df_map['label'] == 0) | (df_map['label'] == 2)]\n",
    "    # df_model = df_model.replace(2.0, 'top')\n",
    "    # df_model = df_model.replace(0, 'botton')\n",
    "    # Reduz o valor da classe maior para um\n",
    "    df_map = df_map.replace(2, 1)\n",
    "    # Seta todos os valores ativados (classe top 1 ou classe botton 0) para 1 e todos os nulos para 0\n",
    "    df_map.iloc[:,1:] = df_map.iloc[:,1:].notnull().astype('int')\n",
    "    \n",
    "    #pd.Series(df_model.iloc[:,2:].isnull().values.all(axis=0)).value_counts()\n",
    "    return df_map\n",
    "\n",
    "def get_embedding(og_df):\n",
    "    # Compila as labes dos textos\n",
    "    df_embedding = og_df['label']\n",
    "\n",
    "    # Junta a esse df os valores de última camada do modelo transpostos em tabela\n",
    "    df_embedding = pd.concat([df_embedding, pd.DataFrame(og_df.iloc[:,-1].to_list())], axis=1)\n",
    "\n",
    "    # Filtra as duas classes de interesse\n",
    "    df_embedding = df_embedding[(df_embedding['label'] == 0) | (df_embedding['label'] == 2)]\n",
    "    # Reduz o valor da classe maior para um\n",
    "    df_embedding['label'] = df_embedding['label'].replace(2.0, 1)\n",
    "    \n",
    "    return df_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b58ba3e-f961-4de8-aa9f-c6351ca9019b",
   "metadata": {},
   "source": [
    "# Executando Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9050409-fd49-4437-ab2b-332d7df52892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: nlpie/tiny-clinicalbert  -  5  -  ?M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/figenio/Projetos/data_science/experimentacao/graph_experimenting/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|███████████████████████████| 447/447 [00:00<00:00, 4667.91 examples/s]\n",
      "Map:   0%|                                       | 0/447 [00:00<?, ? examples/s]Some weights of BertModel were not initialized from the model checkpoint at nlpie/tiny-clinicalbert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|█████████████████████████████| 447/447 [00:16<00:00, 27.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# DEFINIÇÃO DE MODELOS E MAPEAMENTOS\n",
    "models = { \n",
    "    \"nlpie/tiny-clinicalbert\": '?M',          \n",
    "    # \"distilbert-base-uncased\": '67M',\n",
    "    # \"google-bert/bert-base-uncased\": '110M',\n",
    "    # \"emilyalsentzer/Bio_ClinicalBERT\": '110M',\n",
    "    # \"google-bert/bert-large-uncased\": '330M',\n",
    "    \n",
    "    # \"neuralmind/bert-base-portuguese-cased\": \"110M\",\n",
    "    # \"pucpr/biobertpt-all\": \"110M\",\n",
    "    # \"google-bert/bert-base-multilingual-cased\": \"110M\",\n",
    "    # \"neuralmind/bert-large-portuguese-cased\": \"330M\",\n",
    "    # \"pierreguillou/gpt2-small-portuguese\": \"1.5B\",\n",
    "    # \"pucpr/gpt2-bio-pt\": \"1.5B\",\n",
    "    \n",
    "    # \"openai-community/gpt2\": \"1.5B\",\n",
    "    # \"openai-community/gpt2-large\": 36\n",
    "    # \"FacebookAI/xlm-roberta-large\": '560M',\n",
    "    # \"facebook/xlm-roberta-xl\": '3.48B'          # 3.48B\n",
    "}\n",
    "position = 1\n",
    "map_dimensions = [\n",
    "    5,\n",
    "    # 10,\n",
    "    # 25\n",
    "]\n",
    "\n",
    "test_prop = 0.25\n",
    "random = 42\n",
    "components = 20\n",
    "\n",
    "# CRIAÇÃO DE ARQUIVOS\n",
    "df_score = pd.DataFrame(columns = ['model','size','model_size','5x5_mean','5x5_std','10x10_mean','10x10_std','25x25_mean','25x25_std'])\n",
    "# for model_name, model_size in models.items():\n",
    "#     df_score = pd.concat([pd.DataFrame([model_name], columns=[\"model\"]), df_score], ignore_index=True)\n",
    "#     df_score.loc[df_score[\"model\"] == model_name, 'size'] = model_size\n",
    "#     df_score.loc[df_score[\"model\"] == model_name, 'model_size'] = model_name + \" \" + model_size\n",
    "#     df_score.to_csv('data/comparacao_f_pca_pt' + exp + '.csv', index=False)\n",
    "\n",
    "# LEITURA E SOBRESCEVER ARQUIVOS\n",
    "# df_score = pd.read_csv('data/comparacao_f_pca' + lang + exp + '.csv')\n",
    "\n",
    "for model_name, model_size in models.items():\n",
    "    # Cria a linha para o modelo\n",
    "    df_score = pd.concat([pd.DataFrame([model_name], columns=[\"model\"]), df_score], ignore_index=True)\n",
    "    # Atribui valores de tamanho e nome com tamanho\n",
    "    df_score.loc[df_score[\"model\"] == model_name, 'size'] = model_size\n",
    "    df_score.loc[df_score[\"model\"] == model_name, 'model_size'] = model_name + \" \" + model_size\n",
    "    \n",
    "    for dimension in map_dimensions:\n",
    "        # Processando áreas de ativação\n",
    "        print(\"Loading:\", model_name, \" - \", dimension, \" - \", model_size)\n",
    "        llm_mri = LLM_MRI(model=model_name, device=\"cpu\", dataset=subset)\n",
    "        llm_mri.process_activation_areas(map_dimension = dimension)\n",
    "\n",
    "        # Gerando Mapeamento\n",
    "        df_map = get_mapping(llm_mri.reduced_dataset[0]['label'])\n",
    "        # Criando o pca\n",
    "        pca = PCA(n_components=components)\n",
    "\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(df_map.iloc[:,1:], df_map['label'], random_state=random)\n",
    "\n",
    "        # Obtendo classificador\n",
    "        logit_c_scores = cross_val_score(LogisticRegression(), pca.fit_transform(shuffle(df_map.iloc[:,1:], random_state = random)), shuffle(df_map['label'], random_state = random), cv=4, scoring='f1_macro')\n",
    "    \n",
    "        # Montando Embedding\n",
    "        df_emb = get_embedding(llm_mri.hidden_states_dataset.to_pandas())\n",
    "            \n",
    "        # X_train_emb, X_test_emb, y_train_emb, y_test_emb = train_test_split(df_emb.iloc[:,1:], df_emb['label'], random_state=random)\n",
    "            \n",
    "        logit_c_scores_emb = cross_val_score(LogisticRegression(), pca.fit_transform(shuffle(df_emb.iloc[:,1:], random_state = random)), shuffle(df_emb['label'], random_state = random), cv=4, scoring='f1_macro')\n",
    "\n",
    "        # Registra os valores no df\n",
    "        \n",
    "        df_score.loc[df_score[\"model\"] == model_name, str(dimension)+\"x\"+str(dimension)+\"_mean\"] = round(logit_c_scores.mean(), 2)\n",
    "        df_score.loc[df_score[\"model\"] == model_name, str(dimension)+\"x\"+str(dimension)+\"_std\"] = round(logit_c_scores.std(), 2)\n",
    "        df_score.loc[df_score[\"model\"] == model_name, \"embedding_mean\"] = round(logit_c_scores_emb.mean(), 2)\n",
    "        df_score.loc[df_score[\"model\"] == model_name, \"embedding_std\"] = round(logit_c_scores_emb.std(), 2)\n",
    "\n",
    "        # Salva os dados no .csv\n",
    "        # df_score.to_csv('data/comparacao_f_pca' + lang + exp + '.csv', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "781d629e-fedd-45f9-956f-549a26b88089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>0_0_0</th>\n",
       "      <th>0_0_1</th>\n",
       "      <th>0_0_2</th>\n",
       "      <th>0_0_3</th>\n",
       "      <th>0_0_4</th>\n",
       "      <th>0_1_0</th>\n",
       "      <th>0_1_1</th>\n",
       "      <th>0_1_2</th>\n",
       "      <th>0_1_3</th>\n",
       "      <th>...</th>\n",
       "      <th>4_2_3</th>\n",
       "      <th>4_3_1</th>\n",
       "      <th>4_3_2</th>\n",
       "      <th>4_3_3</th>\n",
       "      <th>4_3_4</th>\n",
       "      <th>4_4_0</th>\n",
       "      <th>4_4_1</th>\n",
       "      <th>4_4_2</th>\n",
       "      <th>4_4_3</th>\n",
       "      <th>4_4_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  0_0_0  0_0_1  0_0_2  0_0_3  0_0_4  0_1_0  0_1_1  0_1_2  0_1_3  \\\n",
       "0        1    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1        1    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "5        1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "6        1    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0   \n",
       "11       1    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "433      1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "436      0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "438      0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "440      0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "442      1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "     ...  4_2_3  4_3_1  4_3_2  4_3_3  4_3_4  4_4_0  4_4_1  4_4_2  4_4_3  4_4_4  \n",
       "0    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0  \n",
       "1    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0  \n",
       "5    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0  \n",
       "6    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0  \n",
       "11   ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0  \n",
       "..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "433  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0  \n",
       "436  ...    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0  \n",
       "438  ...    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0  \n",
       "440  ...    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0  \n",
       "442  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0  \n",
       "\n",
       "[178 rows x 68 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f4ba6-c909-4c95-802c-abcbb796e4a3",
   "metadata": {},
   "source": [
    "# Mapping *Evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a3b84e3-de76-42b6-adcd-0f5dac73826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average_function = \"binary\" # \"weighted\" \"binary\" \"macro\" \n",
    "\n",
    "# test_prop = 0.25\n",
    "# random = 42\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df_model.iloc[:,1:], df_model['label'],\n",
    "#                                                     test_size=test_prop, random_state=random)\n",
    "# # Regressão Logística\n",
    "# logit_c_scores = cross_val_score(LogisticRegression(), X_train, y_train, cv=4, scoring='f1_macro')\n",
    "# logit_clf = LogisticRegression().fit(X_train, y_train)\n",
    "# logit_y_pred = pd.Series(logit_clf.predict(X_test))\n",
    "\n",
    "# # Regressão Logística - OVERFITING\n",
    "# logit_l1_c_scores = cross_val_score(LogisticRegression(penalty='l1', solver='saga'), X_train, y_train, cv=4, scoring='f1_macro')\n",
    "# logit_l1_clf = LogisticRegression(penalty='l1', solver='saga').fit(X_train, y_train)\n",
    "# logit_l1_y_pred = pd.Series(logit_l1_clf.predict(X_test))\n",
    "\n",
    "# # Regressão Logística - OVERFITING\n",
    "# logit_l2_c_scores = cross_val_score(LogisticRegression(penalty='l2', solver='saga'), X_train, y_train, cv=4, scoring='f1_macro')\n",
    "# logit_l2_clf = LogisticRegression(penalty='l2', solver='saga').fit(X_train, y_train)\n",
    "# logit_l2_y_pred = pd.Series(logit_l2_clf.predict(X_test))\n",
    "\n",
    "# # Ridge Classificador\n",
    "# ridge_c_scores = cross_val_score(RidgeClassifier(), X_train, y_train, cv=4, scoring='f1_macro')\n",
    "# ridge_clf = RidgeClassifier().fit(X_train, y_train)\n",
    "# ridge_y_pred = pd.Series(ridge_clf.predict(X_test))\n",
    "\n",
    "\n",
    "\n",
    "# print(\" --------- MAPPING f1-core --------- \")\n",
    "# print(\"           Cross-validation\")\n",
    "# print(\"           mean      std\")\n",
    "# print(\"Logit f1:  %0.2f      %0.2f\" % (logit_c_scores.mean(), logit_c_scores.std()))\n",
    "# print(\"Log L1 f1: %0.2f      %0.2f\" % (logit_l1_c_scores.mean(), logit_l1_c_scores.std()))\n",
    "# print(\"Log L2 f1: %0.2f      %0.2f\" % (logit_l2_c_scores.mean(), logit_l2_c_scores.std()))\n",
    "# print(\"Ridge f1:  %0.2f      %0.2f\" % (ridge_c_scores.mean(), ridge_c_scores.std()))\n",
    "# # print(\"Tree f1:   %0.2f      %0.2f      %0.2f\" % (metrics.f1_score(y_test, tree_y_pred, average=average_function), tree_c_scores.mean(), tree_c_scores.std()))\n",
    "# # print(\"Forest f1: %0.2f      %0.2f      %0.2f\" % (metrics.f1_score(y_test, forest_y_pred, average=average_function), forest_c_scores.mean(), forest_c_scores.std()))\n",
    "# # print(\"SVM f1:    %0.2f      %0.2f      %0.2f\" % (metrics.f1_score(y_test, svm_y_pred, average=average_function), svm_c_scores.mean(), svm_c_scores.std()))\n",
    "# # print(\"MLP f1:    %0.2f      %0.2f      %0.2f\" % (metrics.f1_score(y_test, mlp_y_pred, average=average_function), mlp_c_scores.mean(), mlp_c_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc80919-a3b1-4a88-951c-0b647365ee62",
   "metadata": {},
   "source": [
    "# *Embedding Evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c54a87a-a56a-4371-9a33-f49534376f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_embedding = pd.DataFrame(llm_mri.reduced_dataset[0]['label'])\n",
    "\n",
    "# df_embedding = llm_mri.hidden_states_dataset.to_pandas()['label']\n",
    "\n",
    "# df_embedding = pd.concat([df_embedding, pd.DataFrame(llm_mri.hidden_states_dataset.to_pandas()[\"hidden_state_6\"].to_list())], axis=1)\n",
    "\n",
    "\n",
    "# df_embedding = df_embedding[(df_embedding['label'] == 0) | (df_embedding['label'] == 2)]\n",
    "# df_embedding['label'] = df_embedding['label'].replace(2.0, 1)\n",
    "# # df_embedding['label'] = df_embedding['label'].replace(0, 0)\n",
    "# # df_embedding = df_embedding.replace(2, 1)\n",
    "# # df_embedding.iloc[:,1:] = df_embedding.iloc[:,1:].notnull().astype('int')\n",
    "\n",
    "# df_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51e5000b-43c1-41cf-bd01-fd6bd0ca2146",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# average_function = \"binary\" # \"weighted\" \"binary\" \"macro\" \n",
    "\n",
    "# test_prop = 0.25\n",
    "# random = 42\n",
    "# X_train_emb, X_test_emb, y_train_emb, y_test_emb = train_test_split(df_embedding.iloc[:,1:], df_embedding['label'],\n",
    "#                                                     test_size=test_prop, random_state=random)\n",
    "# # Regressão Logística\n",
    "# logit_c_scores = cross_val_score(LogisticRegression(), X_train_emb, y_train_emb, cv=4, scoring='f1_macro')\n",
    "# logit_clf = LogisticRegression().fit(X_train_emb, y_train_emb)\n",
    "# logit_y_pred = pd.Series(logit_clf.predict(X_test_emb))\n",
    "\n",
    "# # Regressão Logística - OVERFITING\n",
    "# logit_l1_c_scores = cross_val_score(LogisticRegression(penalty='l1', solver='saga'), X_train_emb, y_train_emb, cv=4, scoring='f1_macro')\n",
    "# logit_l1_clf = LogisticRegression(penalty='l1', solver='saga').fit(X_train_emb, y_train_emb)\n",
    "# logit_l1_y_pred = pd.Series(logit_l1_clf.predict(X_test_emb))\n",
    "\n",
    "# # Regressão Logística - OVERFITING\n",
    "# logit_l2_c_scores = cross_val_score(LogisticRegression(penalty='l2', solver='saga'), X_train_emb, y_train_emb, cv=4, scoring='f1_macro')\n",
    "# logit_l2_clf = LogisticRegression(penalty='l2', solver='saga').fit(X_train_emb, y_train_emb)\n",
    "# logit_l2_y_pred = pd.Series(logit_l2_clf.predict(X_test_emb))\n",
    "\n",
    "# # Ridge Classificador\n",
    "# ridge_c_scores = cross_val_score(RidgeClassifier(), X_train_emb, y_train_emb, cv=4, scoring='f1_macro')\n",
    "# ridge_clf = RidgeClassifier().fit(X_train_emb, y_train_emb)\n",
    "# ridge_y_pred = pd.Series(ridge_clf.predict(X_test_emb))\n",
    "\n",
    "# # Árvore de decisão\n",
    "# tree_clf = tree.DecisionTreeClassifier(\n",
    "#     criterion=\"entropy\",\n",
    "#     # max_depth=5,\n",
    "#     #min_samples_leaf=50,\n",
    "#     #ax_leaf_nodes=30,\n",
    "#     #class_weight=\"balanced\"\n",
    "# )\n",
    "# tree_c_scores = cross_val_score(tree_clf, X_train_emb, y_train_emb, cv=4, scoring='f1_macro')\n",
    "# tree_clf = tree_clf.fit(X_train_emb, y_train_emb)\n",
    "# tree_y_pred = pd.Series(tree_clf.predict(X_test_emb))\n",
    "\n",
    "\n",
    "# forest_clf = RandomForestClassifier(\n",
    "#     # n_estimators = 100,\n",
    "#     # criterion = 'entropy',\n",
    "#     # max_depth = 5,\n",
    "#     # class_weight= \"balanced\",\n",
    "# )\n",
    "# forest_c_scores = cross_val_score(forest_clf, X_train_emb, y_train_emb, cv=4, scoring='f1_macro')\n",
    "# forest_clf = forest_clf.fit(X_train_emb, y_train_emb)\n",
    "# forest_y_pred = pd.Series(forest_clf.predict(X_test_emb))\n",
    "\n",
    "\n",
    "# svm_clf = svm.SVC()\n",
    "# svm_c_scores = cross_val_score(svm_clf, X_train_emb, y_train_emb, cv=4, scoring='f1_macro')\n",
    "# svm_clf = svm_clf.fit(X_train_emb, y_train_emb)\n",
    "# svm_y_pred = pd.Series(svm_clf.predict(X_test_emb))\n",
    "\n",
    "\n",
    "# mlp_clf = MLPClassifier(\n",
    "#     # hidden_layer_sizes=(32,32,32),\n",
    "#     # activation='relu', # 'identity' 'logistic' 'tanh' 'relu'\n",
    "# #    solver='sgd',\n",
    "# #    alpha=1e-4,\n",
    "# #    learning_rate=\"invscaling\",\n",
    "# #    learning_rate_init = 1e-2,\n",
    "# #    tol=1e-3\n",
    "# )\n",
    "# mlp_c_scores = cross_val_score(mlp_clf, X_train_emb, y_train_emb, cv=4, scoring='f1_macro')\n",
    "# mlp_clf = mlp_clf.fit(X_train_emb, y_train_emb)\n",
    "# mlp_y_pred = pd.Series(mlp_clf.predict(X_test_emb))\n",
    "\n",
    "\n",
    "# print(\" -------- EMBEDDING f1-core -------- \")\n",
    "# print(\"           Cross-validation\")\n",
    "# print(\"           mean      std\")\n",
    "# print(\"Logit f1:  %0.2f      %0.2f\" % (logit_c_scores.mean(), logit_c_scores.std()))\n",
    "# print(\"Log L1 f1: %0.2f      %0.2f\" % (logit_l1_c_scores.mean(), logit_l1_c_scores.std()))\n",
    "# print(\"Log L2 f1: %0.2f      %0.2f\" % (logit_l2_c_scores.mean(), logit_l2_c_scores.std()))\n",
    "# print(\"Ridge f1:  %0.2f      %0.2f\" % (ridge_c_scores.mean(), ridge_c_scores.std()))\n",
    "# # print(\"Tree f1:   %0.2f      %0.2f      %0.2f\" % (metrics.f1_score(y_test_emb, tree_y_pred, average=average_function), tree_c_scores.mean(), tree_c_scores.std()))\n",
    "# # print(\"Forest f1: %0.2f      %0.2f      %0.2f\" % (metrics.f1_score(y_test_emb, forest_y_pred, average=average_function), forest_c_scores.mean(), forest_c_scores.std()))\n",
    "# # print(\"SVM f1:    %0.2f      %0.2f      %0.2f\" % (metrics.f1_score(y_test_emb, svm_y_pred, average=average_function), svm_c_scores.mean(), svm_c_scores.std()))\n",
    "# # print(\"MLP f1:    %0.2f      %0.2f      %0.2f\" % (metrics.f1_score(y_test_emb, mlp_y_pred, average=average_function), mlp_c_scores.mean(), mlp_c_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a396cfff-8a3b-4bd3-9418-126f88813951",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f3f95f7-f293-426e-b10f-2bfaa311493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_prop = 0.25\n",
    "# random = 42\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df_model.iloc[:,1:], df_model['label'],\n",
    "#                                                     test_size=test_prop, random_state=random)\n",
    "\n",
    "# pca = PCA(n_components=20)\n",
    "\n",
    "# logit_c_scores = cross_val_score(LogisticRegression(), pca.fit_transform(X_train), y_train, cv=4, scoring='f1_macro')\n",
    "# logit_clf = LogisticRegression().fit(pca.fit_transform(X_train), y_train)\n",
    "# logit_y_pred = pd.Series(logit_clf.predict(pca.fit_transform(X_test)))\n",
    "\n",
    "# tree_clf = tree.DecisionTreeClassifier(\n",
    "#     criterion=\"entropy\",\n",
    "#     # max_depth=5,\n",
    "#     #min_samples_leaf=50,\n",
    "#     #ax_leaf_nodes=30,\n",
    "#     #class_weight=\"balanced\"\n",
    "# )\n",
    "# tree_c_scores = cross_val_score(tree_clf, pca.fit_transform(X_train), y_train, cv=4, scoring='f1_macro')\n",
    "# tree_clf = tree_clf.fit(pca.fit_transform(X_train), y_train)\n",
    "# tree_y_pred = pd.Series(tree_clf.predict(pca.fit_transform(X_test)))\n",
    "\n",
    "\n",
    "# forest_clf = RandomForestClassifier(\n",
    "#     # n_estimators = 100,\n",
    "#     # criterion = 'entropy',\n",
    "#     # max_depth = 5,\n",
    "#     # class_weight= \"balanced\",\n",
    "# )\n",
    "# forest_clf = forest_clf.fit(pca.fit_transform(X_train), y_train)\n",
    "# forest_y_pred = pd.Series(forest_clf.predict(pca.fit_transform(X_test)))\n",
    "# forest_c_scores = cross_val_score(forest_clf, pca.fit_transform(X_train), y_train, cv=4, scoring='f1_macro')\n",
    "\n",
    "\n",
    "# svm_clf = svm.SVC()\n",
    "# svm_c_scores = cross_val_score(svm_clf, pca.fit_transform(X_train), y_train, cv=4, scoring='f1_macro')\n",
    "# svm_clf = svm_clf.fit(pca.fit_transform(X_train), y_train)\n",
    "# svm_y_pred = pd.Series(svm_clf.predict(pca.fit_transform(X_test)))\n",
    "\n",
    "\n",
    "# mlp_clf = MLPClassifier(\n",
    "#     # hidden_layer_sizes=(32,32,32),\n",
    "#     # activation='relu', # 'identity' 'logistic' 'tanh' 'relu'\n",
    "# #    solver='sgd',\n",
    "# #    alpha=1e-4,\n",
    "# #    learning_rate=\"invscaling\",\n",
    "# #    learning_rate_init = 1e-2,\n",
    "# #    tol=1e-3\n",
    "# )\n",
    "# mlp_c_scores = cross_val_score(mlp_clf, pca.fit_transform(X_train), y_train, cv=4, scoring='f1_macro')\n",
    "# mlp_clf = mlp_clf.fit(pca.fit_transform(X_train), y_train)\n",
    "# mlp_y_pred = pd.Series(mlp_clf.predict(pca.fit_transform(X_test)))\n",
    "\n",
    "\n",
    "\n",
    "# print(\" --------- MAPPING f1-core --------- \")\n",
    "# print(\"           Cross-validation\")\n",
    "# print(\"           mean      std\")\n",
    "# print(\"Logit f1:  %0.2f      %0.2f\" % (logit_c_scores.mean(), logit_c_scores.std()))\n",
    "# print(\"Tree f1:   %0.2f      %0.2f\" % (tree_c_scores.mean(), tree_c_scores.std()))\n",
    "# print(\"Forest f1: %0.2f      %0.2f\" % (forest_c_scores.mean(), forest_c_scores.std()))\n",
    "# print(\"SVM f1:    %0.2f      %0.2f\" % (svm_c_scores.mean(), svm_c_scores.std()))\n",
    "# print(\"MLP f1:    %0.2f      %0.2f\" % (mlp_c_scores.mean(), mlp_c_scores.std()))\n",
    "\n",
    "\n",
    "\n",
    "# X_train_emb, X_test_emb, y_train_emb, y_test_emb = train_test_split(df_embedding.iloc[:,1:], df_embedding['label'],\n",
    "#                                                     test_size=test_prop, random_state=random)\n",
    "# # Regressão Logística\n",
    "# logit_c_scores = cross_val_score(LogisticRegression(),  pca.fit_transform(X_train_emb), y_train_emb, cv=4, scoring='f1_macro')\n",
    "# logit_clf = LogisticRegression().fit(pca.fit_transform(X_train_emb), y_train_emb)\n",
    "# logit_y_pred = pd.Series(logit_clf.predict(pca.fit_transform(X_test_emb)))\n",
    "\n",
    "# # Árvore de decisão\n",
    "# tree_clf = tree.DecisionTreeClassifier(\n",
    "#     criterion=\"entropy\",\n",
    "#     # max_depth=5,\n",
    "#     #min_samples_leaf=50,\n",
    "#     #ax_leaf_nodes=30,\n",
    "#     #class_weight=\"balanced\"\n",
    "# )\n",
    "# tree_c_scores = cross_val_score(tree_clf, pca.fit_transform(X_train_emb), y_train_emb, cv=4, scoring='f1_macro')\n",
    "# tree_clf = tree_clf.fit(pca.fit_transform(X_train_emb), y_train_emb)\n",
    "# tree_y_pred = pd.Series(tree_clf.predict(pca.fit_transform(X_test_emb)))\n",
    "\n",
    "\n",
    "# forest_clf = RandomForestClassifier(\n",
    "#     # n_estimators = 100,\n",
    "#     # criterion = 'entropy',\n",
    "#     # max_depth = 5,\n",
    "#     # class_weight= \"balanced\",\n",
    "# )\n",
    "# forest_c_scores = cross_val_score(forest_clf, pca.fit_transform(X_train_emb), y_train_emb, cv=4, scoring='f1_macro')\n",
    "# forest_clf = forest_clf.fit(pca.fit_transform(X_train_emb), y_train_emb)\n",
    "# forest_y_pred = pd.Series(forest_clf.predict(pca.fit_transform(X_test_emb)))\n",
    "\n",
    "\n",
    "# svm_clf = svm.SVC()\n",
    "# svm_c_scores = cross_val_score(svm_clf, pca.fit_transform(X_train_emb), y_train_emb, cv=4, scoring='f1_macro')\n",
    "# svm_clf = svm_clf.fit(pca.fit_transform(X_train_emb), y_train_emb)\n",
    "# svm_y_pred = pd.Series(svm_clf.predict(pca.fit_transform(X_test_emb)))\n",
    "\n",
    "\n",
    "# mlp_clf = MLPClassifier(\n",
    "#     # hidden_layer_sizes=(32,32,32),\n",
    "#     # activation='relu', # 'identity' 'logistic' 'tanh' 'relu'\n",
    "# #    solver='sgd',\n",
    "# #    alpha=1e-4,\n",
    "# #    learning_rate=\"invscaling\",\n",
    "# #    learning_rate_init = 1e-2,\n",
    "# #    tol=1e-3\n",
    "# )\n",
    "# mlp_c_scores = cross_val_score(mlp_clf, pca.fit_transform(X_train_emb), y_train_emb, cv=4, scoring='f1_macro')\n",
    "# mlp_clf = mlp_clf.fit(pca.fit_transform(X_train_emb), y_train_emb)\n",
    "# mlp_y_pred = pd.Series(mlp_clf.predict(pca.fit_transform(X_test_emb)))\n",
    "\n",
    "\n",
    "# print(\" -------- EMBEDDING f1-core -------- \")\n",
    "# print(\"           Cross-validation\")\n",
    "# print(\"           mean      std\")\n",
    "# print(\"Logit f1:  %0.2f      %0.2f\" % (logit_c_scores.mean(), logit_c_scores.std()))\n",
    "# print(\"Tree f1:   %0.2f      %0.2f\" % (tree_c_scores.mean(), tree_c_scores.std()))\n",
    "# print(\"Forest f1: %0.2f      %0.2f\" % (forest_c_scores.mean(), forest_c_scores.std()))\n",
    "# print(\"SVM f1:    %0.2f      %0.2f\" % (svm_c_scores.mean(), svm_c_scores.std()))\n",
    "# print(\"MLP f1:    %0.2f      %0.2f\" % (mlp_c_scores.mean(), mlp_c_scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3c5aba0-d76c-4c7f-a1a6-724eccc05209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\" --------- MAPPING f1-core --------- \")\n",
    "# print(\"           Cross-validation\")\n",
    "# print(\"           mean      std\")\n",
    "# print(\"Logit f1:  %0.2f      %0.2f\" % (logit_c_scores.mean(), logit_c_scores.std()))\n",
    "# print(\"Log L1 f1: %0.2f      %0.2f\" % (logit_l1_c_scores.mean(), logit_l1_c_scores.std()))\n",
    "# print(\"Log L2 f1: %0.2f      %0.2f\" % (logit_l2_c_scores.mean(), logit_l2_c_scores.std()))\n",
    "# print(\"Ridge f1:  %0.2f      %0.2f\" % (ridge_c_scores.mean(), ridge_c_scores.std()))\n",
    "\n",
    "# print(\" -------- EMBEDDING f1-core -------- \")\n",
    "# print(\"           Cross-validation\")\n",
    "# print(\"           mean      std\")\n",
    "# print(\"Logit f1:  %0.2f      %0.2f\" % (logit_c_scores.mean(), logit_c_scores.std()))\n",
    "# print(\"Log L1 f1: %0.2f      %0.2f\" % (logit_l1_c_scores.mean(), logit_l1_c_scores.std()))\n",
    "# print(\"Log L2 f1: %0.2f      %0.2f\" % (logit_l2_c_scores.mean(), logit_l2_c_scores.std()))\n",
    "# print(\"Ridge f1:  %0.2f      %0.2f\" % (ridge_c_scores.mean(), ridge_c_scores.std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
